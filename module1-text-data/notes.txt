- Pipe in text example
   for doc in tokenizer.pipe(df['reviews.text'], batch_size=500):
        doc_tokens = [token.text for token in doc]
       tokens.append(doc_tokens)
   df['tokens'] = tokens

- Text to tokens
	from spacy.tokenizer import Tokenizer
	nlp = spacy.load('en_core_web_lg')
	tokenizer = Tokenizer(nlp.vocab)
	shops['full_review_tokens'] = shops['full_review_text'].apply(lambda x: tokenizer(x.strip())) 
	shops['full_review_text'] = shop['full_review_tokens'].apply(lambda tokens: [token.text for token in tokens])

- Filter Stopwords
	wc[~wc.word.isin(nlp.Defaults.stop_words)

- 

